# AI â€“ Red Teaming

## ğŸ“Œ IntroduÃ§Ã£o

Este repositÃ³rio Ã© minha contribuiÃ§Ã£o para a comunidade de **Red Teamers**, com foco em **simulaÃ§Ã£o de adversÃ¡rios em IAs (LLMs)**.

O material aqui reunido Ã© resultado de **anotaÃ§Ãµes, estudos e experimentos prÃ¡ticos**, baseados em conteÃºdos pÃºblicos encontrados na internet e em **testes reais realizados contra diferentes modelos de linguagem**. O foco nÃ£o Ã© teoria abstrata, mas sim **o que funcionou na prÃ¡tica** durante avaliaÃ§Ãµes de comportamento e resiliÃªncia de IAs.

O objetivo principal Ã© documentar tÃ©cnicas, padrÃµes e abordagens que permitam:

* Avaliar limites e restriÃ§Ãµes de LLMs
* Simular comportamentos adversariais
* Identificar falhas de alinhamento
* Explorar tÃ©cnicas de **jailbreak** sob a Ã³tica de Red Team

---

## ğŸ¯ Objetivo do RepositÃ³rio

Este repositÃ³rio tem carÃ¡ter **educacional e de pesquisa**, sendo voltado para:

* Profissionais de **Red Team**
* Pesquisadores em **SeguranÃ§a de IA**
* Pessoas interessadas em **LLM Security, Prompt Injection e Adversarial Testing**

O conteÃºdo aqui apresentado deve ser utilizado **exclusivamente para fins de estudo, pesquisa e fortalecimento da seguranÃ§a de sistemas baseados em IA**.

---

## ğŸ§  Metodologias Abordadas

Abaixo estÃ£o as principais metodologias e tÃ©cnicas que serÃ£o documentadas neste repositÃ³rio.

### ğŸ”¹ Jailbreak via Crescendo

TÃ©cnica baseada em **escalonamento progressivo de contexto**, onde o prompt comeÃ§a de forma inofensiva e, gradualmente, conduz o modelo a estados de resposta mais permissivos.

Essa abordagem explora:

* AcÃºmulo de contexto
* ConfianÃ§a progressiva do modelo
* Quebra gradual de restriÃ§Ãµes

---

### ğŸ”¹ Skeleton Key

Metodologia que utiliza **estruturas-base (esqueletos de instruÃ§Ãµes)** para induzir o modelo a aceitar comandos sensÃ­veis como parte de um processo maior, legÃ­timo ou tÃ©cnico.

Muito eficaz para:

* Bypass de filtros diretos
* ReformulaÃ§Ã£o de intenÃ§Ãµes proibidas
* Engenharia de prompts complexos

---

### ğŸ”¹ Prompt Injection

ExploraÃ§Ã£o de falhas na interpretaÃ§Ã£o de instruÃ§Ãµes do modelo, manipulando entradas para **subverter regras, polÃ­ticas ou contexto previamente definido**.

Inclui cenÃ¡rios como:

* InjeÃ§Ã£o direta
* InjeÃ§Ã£o indireta
* Quebra de contexto
* Conflito de instruÃ§Ãµes

---

### ğŸ”¹ Persona e Personalidade

CriaÃ§Ã£o e manipulaÃ§Ã£o de **personas artificiais** para alterar o comportamento do modelo, levando-o a agir fora de suas restriÃ§Ãµes normais.

Essa tÃ©cnica explora:

* Roleplay avanÃ§ado
* Desvio de alinhamento
* Contextos alternativos de autoridade ou especializaÃ§Ã£o

---

## ğŸ§ª Estrutura do RepositÃ³rio (em evoluÃ§Ã£o)

A organizaÃ§Ã£o do repositÃ³rio seguirÃ¡ uma estrutura semelhante a:

```
.
â”œâ”€â”€ crescendo/
â”œâ”€â”€ skeleton-key/
â”œâ”€â”€ prompt-injection/
â”œâ”€â”€ persona-personality/
â”œâ”€â”€ notes/
â””â”€â”€ references/
```

Cada diretÃ³rio conterÃ¡:

* ExplicaÃ§Ã£o da tÃ©cnica
* Exemplos de prompts
* ObservaÃ§Ãµes prÃ¡ticas
* LimitaÃ§Ãµes e comportamentos observados

---

## âš ï¸ Aviso Importante

Este repositÃ³rio **nÃ£o incentiva uso malicioso** de tÃ©cnicas contra sistemas reais sem autorizaÃ§Ã£o.

Todas as tÃ©cnicas documentadas tÃªm como finalidade:

* Pesquisa
* SimulaÃ§Ã£o adversarial
* Melhoria de seguranÃ§a
* ConscientizaÃ§Ã£o sobre riscos em LLMs

Utilize este conteÃºdo de forma **Ã©tica e responsÃ¡vel**.

---

## ğŸ¤ ContribuiÃ§Ãµes

SugestÃµes, correÃ§Ãµes e contribuiÃ§Ãµes sÃ£o bem-vindas, desde que alinhadas com o propÃ³sito educacional e de pesquisa do projeto.

---

## ğŸ“š ReferÃªncias

As referÃªncias utilizadas serÃ£o adicionadas progressivamente ao longo do repositÃ³rio, sempre que aplicÃ¡vel.

---

ğŸ§  *Think like an adversary. Build safer AI.*
